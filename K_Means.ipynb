{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K-Means",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "6F4SAHaS7_jl",
        "outputId": "3decf995-c020-44a3-d660-9487b25be592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c48c2cc372d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombinations_with_replacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ABCD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mwss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "boston=load_boston()\n",
        "\n",
        "ds=pd.DataFrame(boston.data,columns=boston.feature_names)\n",
        "ds.head()\n",
        "\n",
        "#1-hot encoding of RAD variable; because its categorical variable\n",
        "#representing it as categorical variable\n",
        "ds[\"RAD\"]=ds[\"RAD\"].astype(\"category\")\n",
        "#datatype of the ds\n",
        "ds.dtypes\n",
        "\n",
        "#now using df.get_dummies(); it will drop the original column also\n",
        "#this method will automatically pick the categorical variable and apply 1-hot encoding\n",
        "ds=pd.get_dummies(ds,prefix=\"RAD\")\n",
        "ds.head()\n",
        "\n",
        "#now doing Scaling on AGE,TAX,B or on entire Dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler();\n",
        "scaler=scaler.fit(ds)\n",
        "\n",
        "scaledData=scaler.transform(ds)\n",
        "\n",
        "#now create the scaled dataframe from it\n",
        "dss=pd.DataFrame(scaledData,columns=ds.columns)\n",
        "\n",
        "#now perform the clusetring \n",
        "#step 1  cluster configuration to kind the k\n",
        "#step 2 using the value of 'k', generate the cluster\n",
        "\n",
        "#now to know the best value of 'k' \n",
        "# wss/bss vs k\n",
        "\n",
        "#That is when k=2, wss=sum of all point with theri 2 centeroid individually \n",
        "#        i.e within clusterdistance ( this is inertia )\n",
        "#    and   bwss means distance between centroid c1 and c2\n",
        "\n",
        "#now when k=3, wss= sum of distance all point of culter and their centroid \n",
        "# the above wss is given by inertia of the cluster configuration\n",
        "## but for bwss the sum of distance between 3 centroid.\n",
        "## c1 to c2, c1 to c3 and c2 to c3\n",
        "\n",
        "###when cluster configuration=4\n",
        "##the bss= dist(c1,c2)+dist(c1,c3) +dist(c1,c4) + dist(c2,c3) +dist(c2,c4) +dist(c3,c4)\n",
        "\n",
        "#so all possible combination we need to find out for all values of k\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from itertools import combinations_with_replacement\n",
        "\n",
        "from itertools import combinations \n",
        "from scipy.spatial import distance\n",
        "print(list(combinations_with_replacement(\"ABCD\", 2)))\n",
        "\n",
        "wss=[]\n",
        "bss=[]\n",
        "pairmap={}\n",
        "dis=[]\n",
        "d=0\n",
        "distanceMap={}\n",
        "for k in range(2,16):\n",
        "    #perforiming  the cluster configuration\n",
        "    clust=KMeans(n_clusters=k,random_state=0).fit(dss)\n",
        "    wss.append(clust.inertia_)\n",
        "    c=list(combinations(range(0,k), 2))\n",
        "    print(\"Combinations ----------->\",c)\n",
        "    print(\"ClusterCenters Are Below----------->\")\n",
        "    dataFrameClusterCenter=pd.DataFrame(clust.cluster_centers_)\n",
        "    print(pd.DataFrame(clust.cluster_centers_))\n",
        "    print(\"The above are clusterCenters are for k==\",k)\n",
        "    pairmap[k]={\"pairs\":c}\n",
        "    for i in c:\n",
        "        #converting the tuple() to list using the list() method\n",
        "        pair=list(i)\n",
        "        print(\"pair is\",pair)\n",
        "        #extracting the index from the pair\n",
        "        index1=pair[0]\n",
        "        index2=pair[1]\n",
        "        #print(\"row 1\"); print(dataFrameClusterCenter.iloc[index1,:])\n",
        "        #print(\"row 2\"); print(dataFrameClusterCenter.iloc[index2,:])\n",
        "        d=distance.euclidean(dataFrameClusterCenter.iloc[index1,:],\n",
        "                             dataFrameClusterCenter.iloc[index2,:])\n",
        "        print(\"distance\",d)\n",
        "        #appending the calculated distance between each pair of the cluster centers in a list\n",
        "        dis.append(d)  \n",
        "        distanceMap[k]={\"distance\":dis}\n",
        "    #making the list empty for next k\n",
        "    dis=[]\n",
        "        \n",
        "print(\"disstacne map for each k \")\n",
        "print(distanceMap)   \n",
        "print(\"wss for all k \")\n",
        "print(wss)     \n",
        "\n",
        "\n",
        "#sum the distance of between every cluster \n",
        "#summedDistance storing to bss list\n",
        "bss=[]\n",
        "import math\n",
        "for i in range(2,16):\n",
        "    value=distanceMap.get(i)\n",
        "    print(value)\n",
        "    list=value['distance']\n",
        "    print(math.fsum(list))\n",
        "    summedDistance=math.fsum(list)\n",
        "    bss.append(summedDistance)\n",
        "    \n",
        "bss\n",
        "#1. now we have bss for all the k \n",
        "bss\n",
        "#2. now we have wss for all the k\n",
        "wss\n",
        "#but wss shal be sqrt(wss[i])\n",
        "len(wss)\n",
        "len(bss)\n",
        "sqrtwss=[]\n",
        "for i in range(0,len(wss)):\n",
        "    sqrt=math.sqrt(wss[i])\n",
        "    print(sqrt)\n",
        "    sqrtwss.append(sqrt)\n",
        "\n",
        "#so this sqrtwss shall be used\n",
        "sqrtwss\n",
        "\n",
        "\n",
        "#final ratio =sqrtwss/bss\n",
        "ratio=[]\n",
        "for i in range(0,len(sqrtwss)):\n",
        "    #ratio.append(sqrtwss[i]/wss[i])\n",
        "    ratio.append(sqrtwss[i]/bss[i])\n",
        "    \n",
        "    #So finally perforimg scatter plot of ratio vs k plot\n",
        "#########################   ratio=(sqrtwss/bss) vs k plot ############################\n",
        "ratio\n",
        "del list\n",
        "k=range(2,16)\n",
        "k\n",
        "k=list(k)\n",
        "k\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(k,ratio)\n",
        "plt.xlabel(\"No of cluster k\")\n",
        "plt.ylabel(\"Ratio of sqrtwss/bss\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#plot of sqrtwss vs k\n",
        "plt.plot(k,sqrtwss)\n",
        "plt.xlabel(\"No of cluster k\")\n",
        "plt.ylabel(\"wss or sqrtwss\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#plot of bss vs k\n",
        "plt.plot(k,bss)\n",
        "plt.xlabel(\"No of cluster k\")\n",
        "plt.ylabel(\"bss\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############# Now as we knoe the optiomal value of k is 4, so \n",
        "############# So we now perform actual clustering of 506 observations and there scaled \n",
        "############ scaled and linear independence dataset\n",
        "\n",
        "#our scaled dataset is represented by dss\n",
        "dss.shape\n",
        "#to find corelation matrix \n",
        "dss.corr()\n",
        "\n",
        "\n",
        "#now performing the clustering\n",
        "clust=KMeans(n_clusters=4,max_iter=500,random_state=0).fit(dss)\n",
        "\n",
        "#now extract the clusterCenters\n",
        "clusterCenter=clust.cluster_centers_\n",
        "\n",
        "#convert clusterCenter to dataframe to do the cluster profilin\n",
        "ccd=pd.DataFrame(clusterCenter,columns=dss.columns)\n",
        "\n",
        "#ccd for cluster profilin\n",
        "ccd\n",
        "#so profiling details\n",
        "#clusterId 1 is having the highest crime rate\n",
        "# industry are more in clusterId 1              \n",
        "\n",
        "\n",
        "#to see the labels i.e clusterId for each observation\n",
        "labels=clust.labels_\n",
        "\n",
        "#total labes;\n",
        "len(labels)\n",
        "clusterIds=list(labels)\n",
        "\n",
        "#now perform the inverse Scaling\n",
        "originalDataAsNumpy=scaler.inverse_transform(dss)\n",
        "#converting numpy to dataset\n",
        "originalDataset=pd.DataFrame(originalDataAsNumpy,columns=dss.columns)\n",
        "\n",
        "#adding the labelled column to the originalDataset\n",
        "originalDataset[\"Label\"]=labels\n",
        "\n",
        "#saving data on the system as OriginalData.csv\n",
        "originalDataset.to_csv(\"yoursystem path\\\\originalData.csv\")\n",
        "#to see whether data contains the label or not\n",
        "originalDataset.Label[0]\n",
        "\n",
        "##### Now plotting the Classfication \n",
        "import pylab as pl\n",
        "len=originalDataset.shape[0]\n",
        "len\n",
        "for i in range(0, len):\n",
        "   if originalDataset.Label[i] == 0:\n",
        "      c1 = pl.scatter(originalDataset.iloc[i,2],originalDataset.iloc[i,4],c='r', marker='+')\n",
        "   elif originalDataset.Label[i]  == 1:\n",
        "      c2 = pl.scatter(originalDataset.iloc[i,2],originalDataset.iloc[i,4],c='g',marker='o')\n",
        "   elif originalDataset.Label[i]  == 2:\n",
        "      c3 = pl.scatter(originalDataset.iloc[i,2],originalDataset.iloc[i,4],c='b',marker='*')\n",
        "   elif originalDataset.Label[i] == 3:\n",
        "      c4 = pl.scatter(originalDataset.iloc[i,2],originalDataset.iloc[i,4],c='y',marker='^')\n",
        "pl.legend([c1, c2, c3,c4], ['c1','c2','c3','c4'])  \n",
        "pl.title('Boston Data classification')\n",
        "pl.show()"
      ]
    }
  ]
}